{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we would like to use the CNN, RNN, and LSTM to build the model of image label generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token_pt = 'Flickr8k/Flickr8k.lemma.token.txt'\n",
    "train_pt = 'Flickr8k/Flickr_8k.trainImages.txt' \n",
    "test_pt = 'Flickr8k/Flickr_8k.testImages.txt'\n",
    "glove_pt = 'Flickr8k/glove.6B.200d.txt'\n",
    "image_pt = 'Flicker8k_Dataset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doc = open(token_pt,'r').read()\n",
    "print(doc[:1000])   #example of the image id and description, and find that for each image, there are 5 captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#create a dictionary containing the image id and list of all the 5 captions for it.\n",
    "\n",
    "descriptions = dict()\n",
    "for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        if len(line) > 2:\n",
    "            image_id = tokens[0].split('.')[0]\n",
    "            image_desc = ' '.join(tokens[1:])\n",
    "            if image_id not in descriptions: #merge the captions for same id\n",
    "                descriptions[image_id] = list()\n",
    "            descriptions[image_id].append(image_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dict(list(descriptions.items())[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import string\n",
    "from spacy.lang.en import stop_words as spacy_stopwords\n",
    "\n",
    "punc = string.punctuation\n",
    "stopwd =  spacy_stopwords.STOP_WORDS\n",
    "\n",
    "for key, desc_list in descriptions.items():\n",
    "    for i in range(len(desc_list)):\n",
    "        dc = desc_list[i]\n",
    "        dc = dc.split()\n",
    "        dc = [word.lower() for word in dc]\n",
    "        dc = [word for word in dc if word not in stopwd and word not in punc]\n",
    "        desc_list[i] =  ' '.join(dc)\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dict(list(descriptions.items())[:2]) #we can see that the words are converted to lowercase and punctuations are removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#save the cleaned description into original format\n",
    "lines = list()\n",
    "for key, desc_list in descriptions.items():\n",
    "    for desc in desc_list:\n",
    "        lines.append(key + ' ' + desc)\n",
    "new_descriptions = '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(new_descriptions[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#visulize the image with its formatted descriptions\n",
    "import matplotlib.pyplot as plt\n",
    "pic = '1305564994_00513f9a5b.jpg'\n",
    "x = plt.imread(image_pt+pic)\n",
    "plt.imshow(x)\n",
    "plt.show()\n",
    "descriptions['1305564994_00513f9a5b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#load the train data\n",
    "\n",
    "doc_train = open(train_pt,'r').read()\n",
    "dataset = list()\n",
    "for line in doc_train.split('\\n'):\n",
    "    if len(line) > 1:\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "\n",
    "train = set(dataset)\n",
    "\n",
    "#load the test data\n",
    "doc_test = open(test_pt,'r').read()\n",
    "dataset = list()\n",
    "for line in doc_test.split('\\n'):\n",
    "    if len(line) > 1:\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "\n",
    "test = set(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#locate the train and test image path based on the set information from above cell\n",
    "import glob\n",
    "\n",
    "img = glob.glob(image_pt + '*.jpg')#give the path for all the images\n",
    "\n",
    "#get all paths of the image for training\n",
    "train_image = set(open(train_pt, 'r').read().strip().split('\\n'))\n",
    "train_image_collection=[]\n",
    "for i in img:\n",
    "    if i[len(image_pt):] in train_image:\n",
    "        train_image_collection.append(i)\n",
    "        \n",
    "#get all paths of the image for testing\n",
    "test_image = set(open(test_pt, 'r').read().strip().split('\\n'))\n",
    "test_image_collection=[]\n",
    "for i in img:\n",
    "    if i[len(image_pt):] in test_image:\n",
    "        test_image_collection.append(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#get the descriptions of train images\n",
    "train_desc = {}\n",
    "\n",
    "for line in new_descriptions.split('\\n'):\n",
    "    tokens = line.split()\n",
    "    image_id, image_desc = tokens[0], tokens[1:]\n",
    "    if image_id in train:\n",
    "        if image_id not in train_desc:\n",
    "            train_desc[image_id] = list()\n",
    "        desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "        train_desc[image_id].append(desc)\n",
    "            \n",
    "#view the top 5 lines in the train description\n",
    "dict(list(train_desc.items())[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_train_captions = []\n",
    "for key, val in train_desc.items():\n",
    "    for cap in val:\n",
    "        all_train_captions.append(cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Create the vocabulary for train images\n",
    "\n",
    "Vocab = set()\n",
    "for key in train_desc:\n",
    "        [Vocab.update(d.split()) for d in descriptions[key]]\n",
    "        \n",
    "print('Original Vocabulary Size: %d' % len(Vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#decrease the vocabulary by keeping those words have more than 15 frequency\n",
    "word_freq_thd = 15\n",
    "word_freq = {}\n",
    "nsents = 0\n",
    "for sent in all_train_captions:\n",
    "    nsents += 1\n",
    "    for w in sent.split(' '):\n",
    "        word_freq[w] = word_freq.get(w, 0) + 1\n",
    "vocab = [w for w in word_freq if word_freq[w] >= word_freq_thd]\n",
    "\n",
    "print('Vocabulary = %d' % (len(vocab))) #we are able to find the vocab has decreased from 2320 to 521"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ixtoword = {}\n",
    "wordtoix = {}\n",
    "ix = 1\n",
    "for w in vocab:\n",
    "    wordtoix[w] = ix\n",
    "    ixtoword[ix] = w\n",
    "    ix += 1\n",
    "\n",
    "vocab_size = len(ixtoword) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#find the max length of the description\n",
    "all_desc = list()\n",
    "for key in train_desc.keys():\n",
    "    [all_desc.append(d) for d in train_desc[key]]\n",
    "lines = all_desc\n",
    "max_length = max(len(d.split()) for d in lines)\n",
    "\n",
    "print('Description Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_index = {} \n",
    "f = open(glove_pt, encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#make an matrix for the vocabulary\n",
    "embedding_matrix = np.zeros((vocab_size, 200))\n",
    "for word, i in wordtoix.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip3 install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Using the pretrained network InceptionV3\n",
    "from tensorflow import keras\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "original_model = InceptionV3(weights='imagenet')\n",
    "\n",
    "#remove the classify layer in the InceptionV3\n",
    "from keras.models import Model\n",
    "\n",
    "model = Model(original_model.input, original_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#define the function to reshape the image for InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "def preprocess(image_path):\n",
    "    img = image.load_img(image_path, target_size=(299, 299))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#define the function to extract image vectors\n",
    "def encode(image):\n",
    "    image = preprocess(image) \n",
    "    fea_vec = model.predict(image) \n",
    "    fea_vec = np.reshape(fea_vec, fea_vec.shape[1])\n",
    "    return fea_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#extract the vectors from train and test images\n",
    "\n",
    "##train data\n",
    "train_vecs = {}\n",
    "for img in train_image_collection:\n",
    "    train_vecs[img[len(image_pt):]] = encode(img)\n",
    "\n",
    "\n",
    "##test data\n",
    "test_vecs = {}\n",
    "for img in test_image_collection:\n",
    "    test_vecs[img[len(image_pt):]] = encode(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras.layers import LSTM, Embedding, Dense, Activation, Flatten, Reshape, Dropout\n",
    "from keras import Input, layers\n",
    "from keras.layers.merge import add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#build the feature extractor model\n",
    "inputs1 = Input(shape=(2048,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "#build the description sequence model\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, 200, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "#decoder model\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "#merge two models\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.summary()\n",
    "\n",
    "# use pre-fixed weights for embeddding layer and not trainable.\n",
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False\n",
    "# model compile\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define the data generator function\n",
    "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n=0\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            n+=1\n",
    "            # retrieve the photo feature\n",
    "            photo = photos[key+'.jpg']\n",
    "            for desc in desc_list:\n",
    "                # encode the sequence\n",
    "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
    "                # split one sequence into multiple X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pair\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    # store\n",
    "                    X1.append(photo)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            # yield the batch data\n",
    "            if n==num_photos_per_batch:\n",
    "                yield [[np.array(X1), np.array(X2)], np.array(y)]\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#train the model with 5 epochs due to the storage\n",
    "epochs = 5\n",
    "batch_size = 3\n",
    "steps = len(train_desc)//batch_size\n",
    "\n",
    "generator = data_generator(train_desc, train_vecs, wordtoix, max_length, batch_size)\n",
    "model.fit(generator, epochs=epochs, steps_per_epoch=steps, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the next word prediction based on the greedy search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def greedySearch(photo):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = ixtoword[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "\n",
    "    final = in_text.split()\n",
    "    final = final[1:-1]\n",
    "    final = ' '.join(final)\n",
    "    return final\n",
    "\n",
    "def beam_search_predictions(image, beam_index = 3):\n",
    "    start = [wordtoix[\"startseq\"]]\n",
    "    start_word = [[start, 0.0]]\n",
    "    while len(start_word[0][0]) < max_length:\n",
    "        temp = []\n",
    "        for s in start_word:\n",
    "            par_caps = sequence.pad_sequences([s[0]], maxlen=max_length, padding='post')\n",
    "            preds = model.predict([image,par_caps], verbose=0)\n",
    "            word_preds = np.argsort(preds[0])[-beam_index:]\n",
    "            # Getting the top <beam_index>(n) predictions and creating a \n",
    "            # new list so as to put them via the model again\n",
    "            for w in word_preds:\n",
    "                next_cap, prob = s[0][:], s[1]\n",
    "                next_cap.append(w)\n",
    "                prob += preds[0][w]\n",
    "                temp.append([next_cap, prob])\n",
    "                    \n",
    "        start_word = temp\n",
    "        # Sorting according to the probabilities\n",
    "        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n",
    "        # Getting the top words\n",
    "        start_word = start_word[-beam_index:]\n",
    "    \n",
    "    start_word = start_word[-1][0]\n",
    "    intermediate_caption = [ixtoword[i] for i in start_word]\n",
    "    final_caption = []\n",
    "    for i in intermediate_caption:\n",
    "        if i != 'endseq':\n",
    "            final_caption.append(i)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    final_caption = ' '.join(final_caption[1:])\n",
    "    return final_caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#pick up a image from test data\n",
    "import matplotlib.pyplot as plt\n",
    "pic = '241346971_c100650320.jpg'\n",
    "x = plt.imread(image_pt+pic)\n",
    "plt.imshow(x)\n",
    "plt.show()\n",
    "descriptions['1305564994_00513f9a5b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pic = '241346971_c100650320.jpg'\n",
    "image = test_vecs[pic].reshape((1,2048))\n",
    "x=plt.imread(image_pt+pic)\n",
    "plt.imshow(x)\n",
    "plt.show()\n",
    "\n",
    "print(\"Greedy Search:\",greedySearch(image))\n",
    "print(\"Beam Search, K = 3:\",beam_search_predictions(image, beam_index = 3))\n",
    "print(\"Beam Search, K = 5:\",beam_search_predictions(image, beam_index = 5))\n",
    "print(\"Beam Search, K = 7:\",beam_search_predictions(image, beam_index = 7))\n",
    "print(\"Beam Search, K = 10:\",beam_search_predictions(image, beam_index = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
